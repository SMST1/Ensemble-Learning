{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "*1.  Can we use Bagging for regression problems ?*\n",
        "\n",
        "Yes, Bagging (Bootstrap Aggregating) can definitely be used for regression problems.\n",
        "\n",
        "\n",
        "Bagging involves training multiple models (usually the same type, like decision trees) on different bootstrapped subsets of the training data.\n",
        "\n",
        "Each model makes its own prediction.\n",
        "\n",
        "For regression, the final prediction is typically the average of the individual model predictions.\n",
        "\n",
        "*2.  What is the difference between multiple model training and single model training?*\n",
        "\n",
        "The difference between multiple model training and single model training lies in how many models are used to learn from data and make predictions. Here's a clear breakdown:\n",
        "\n",
        "üîπ Single Model Training\n",
        "Definition:\n",
        "You train one model on the training data and use it for prediction.\n",
        "\n",
        "Example:\n",
        "Training a single decision tree, linear regression model, or neural network.\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "Simple and easy to implement.\n",
        "\n",
        "Faster training and inference time.\n",
        "\n",
        "Limited to the strengths and weaknesses of that one model.\n",
        "\n",
        "Can overfit or underfit depending on the model's complexity.\n",
        "\n",
        "Use case:\n",
        "\n",
        "When the data is clean and the problem is well understood.\n",
        "\n",
        "When interpretability or speed is important.\n",
        "\n",
        "üîπ Multiple Model Training (Ensemble Learning)\n",
        "Definition:\n",
        "You train several models and combine their predictions to make a final decision.\n",
        "\n",
        "Types:\n",
        "\n",
        "Bagging (e.g., Random Forest): Reduces variance.\n",
        "\n",
        "Boosting (e.g., XGBoost, AdaBoost): Reduces bias.\n",
        "\n",
        "Stacking: Combines multiple different models.\n",
        "\n",
        "Characteristics:\n",
        "\n",
        "More robust and accurate than a single model.\n",
        "\n",
        "Reduces overfitting by aggregating diverse predictions.\n",
        "\n",
        "Increases computational cost (more models to train and maintain).\n",
        "\n",
        "Use case:\n",
        "\n",
        "When high accuracy is critical.\n",
        "\n",
        "When the data is noisy or complex.\n",
        "\n",
        "In competitions (like Kaggle) and real-world deployments.\n",
        "\n",
        "*3.  Explain the concept of feature randomness in Random Forest?*\n",
        "\n",
        "Feature randomness is a key concept that makes Random Forests more powerful than basic bagging with decision trees.\n",
        "\n",
        "In a Random Forest:\n",
        "\n",
        "At each split of a decision tree, the algorithm does NOT consider all features.\n",
        "\n",
        "Instead, it randomly selects a subset of features and finds the best split only among those.\n",
        "\n",
        "This is called feature randomness, or feature bagging.\n",
        "\n",
        "If all trees consider all features at every split, they might:\n",
        "\n",
        "Choose the same dominant features over and over.\n",
        "\n",
        "End up making similar trees despite different training samples (bootstrapping).\n",
        "\n",
        "Result in a high correlation among trees, which limits the benefit of averaging.\n",
        "\n",
        "By introducing feature randomness:\n",
        "\n",
        "Each tree is forced to consider different patterns.\n",
        "\n",
        "The ensemble becomes more diverse, which improves generalization.\n",
        "\n",
        "*4.  What is OOB (Out-of-Bag) Score?*\n",
        "\n",
        "The OOB (Out-of-Bag) score is an internal validation method used in ensemble methods like Random Forests, which rely on bootstrap sampling.\n",
        "\n",
        "The OOB score is an estimate of the model's performance calculated using only the out-of-bag samples:\n",
        "\n",
        "For each training instance, collect predictions from the subset of trees that did not see that instance during training.\n",
        "\n",
        "Aggregate those predictions (e.g., majority vote for classification or average for regression).\n",
        "\n",
        "Compare them to the true labels.\n",
        "\n",
        "Compute accuracy (classification) or R¬≤ score (regression) using these OOB predictions.\n",
        "\n",
        "*5.  How can you measure the importance of features in a Random Forest model?*\n",
        "\n",
        "Random Forests naturally provide a way to estimate feature importance, which tells you how valuable each feature is in making predictions.\n",
        "\n",
        "1. Gini Importance (Mean Decrease in Impurity)\n",
        " What is it?\n",
        "Based on how much each feature reduces impurity (e.g., Gini index or variance) across all trees.\n",
        "\n",
        "Features used in high-impact splits near the root of trees contribute more.\n",
        "\n",
        " In scikit-learn:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get importance scores\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# View as a DataFrame\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "print(feature_importance_df)\n",
        " Pros:\n",
        "Fast and built-in.\n",
        "\n",
        "Useful for quick insights.\n",
        "\n",
        " Cons:\n",
        "Can be biased toward features with more categories or higher cardinality.\n",
        "\n",
        " 2. Permutation Importance\n",
        " What is it?\n",
        "Measures the drop in model performance when a feature's values are randomly shuffled.\n",
        "\n",
        "If shuffling a feature significantly harms the model, that feature is important.\n",
        "\n",
        " In scikit-learn:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "result = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)\n",
        "perm_df = pd.DataFrame({\n",
        "    'Feature': X_test.columns,\n",
        "    'Importance': result.importances_mean\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "print(perm_df)\n",
        "Pros:\n",
        "Model-agnostic and more reliable, especially with correlated features.\n",
        "\n",
        "Reflects true predictive power.\n",
        "\n",
        " Cons:\n",
        "Slower, especially on large datasets.\n",
        "\n",
        " 3. SHAP Values (SHapley Additive exPlanations)\n",
        "\n",
        "Uses game theory to explain each feature‚Äôs contribution to every single prediction.\n",
        "\n",
        "Provides local and global interpretability.\n",
        "\n",
        "‚öôÔ∏è Example:\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "import shap\n",
        "\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer.shap_values(X_test)\n",
        "shap.summary_plot(shap_values, X_test)\n",
        " Pros:\n",
        "Very detailed and accurate.\n",
        "\n",
        "Useful for individual predictions and overall model behavior.\n",
        "\n",
        " Cons:\n",
        "More complex to implement.\n",
        "\n",
        "Slower than basic importance methods.\n",
        "\n",
        "*6.  Explain the working principle of a Bagging Classifier?*\n",
        "\n",
        "Bagging stands for Bootstrap Aggregating, and a Bagging Classifier is an ensemble method that improves the stability and accuracy of machine learning algorithms ‚Äî especially unstable ones like decision trees.\n",
        "\n",
        "Working of a Bagging Classifier:\n",
        "1. Bootstrap Sampling (Random Subsets)\n",
        "From the original training data of size N, B new datasets (called bootstrap samples) are created by random sampling with replacement.\n",
        "\n",
        "Each subset is typically the same size N, but it may have duplicates and omit some original samples.\n",
        "\n",
        "2. Train Multiple Base Models\n",
        "A base classifier (like a Decision Tree, SVM, etc.) is trained on each bootstrap sample independently.\n",
        "\n",
        "This results in multiple trained models (usually the same type, but trained on different data).\n",
        "\n",
        "3. Voting for Final Prediction\n",
        "When predicting, each model gives its own output.\n",
        "\n",
        "For classification:\n",
        "\n",
        "Majority Voting is used ‚Äî the class that gets the most votes is the final prediction.\n",
        "\n",
        "For regression (Bagging Regressor), the predictions are averaged.\n",
        "\n",
        "*7.  How do you evaluate a Bagging Classifier‚Äôs performance ?*\n",
        "\n",
        "Evaluating a Bagging Classifier is similar to evaluating any supervised classification model. You assess how well it generalizes to unseen data using metrics based on its predictions.\n",
        "\n",
        "1. Train-Test Split\n",
        "Use a separate test set to evaluate how well the model performs on unseen data.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "‚öôÔ∏è 2. Fit the Bagging Classifier\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "üìä 3. Prediction and Evaluation Metrics\n",
        "‚úÖ Accuracy\n",
        "The percentage of correctly classified instances.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "üè∑Ô∏è Classification Report\n",
        "Includes precision, recall, F1-score for each class.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred))\n",
        "üìâ Confusion Matrix\n",
        "Gives a detailed view of correct vs incorrect predictions.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "üìà ROC AUC Score (for binary classifiers)\n",
        "Useful for evaluating probabilistic predictions and how well the classifier separates classes.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_proba))\n",
        "üîÅ 4. Cross-Validation (Recommended)\n",
        "To get a more reliable performance estimate, especially on small datasets.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
        "print(\"Cross-validated accuracy:\", scores.mean())\n",
        "üîç 5. OOB Score (Out-of-Bag Score)\n",
        "If you're using a BaggingClassifier with bootstrap=True, you can set oob_score=True to get a built-in performance estimate.\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "model = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=100,\n",
        "    oob_score=True,\n",
        "    random_state=42\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "print(\"OOB Score:\", model.oob_score_)\n",
        "\n",
        "\n",
        "*8.  How does a Bagging Regressor work?*\n",
        "\n",
        "A Bagging Regressor is an ensemble method that improves the performance and stability of regression models by combining predictions from multiple models trained on random subsets of the data.\n",
        "\n",
        "It follows the same Bagging (Bootstrap Aggregating) principle used in classification but adapted for regression tasks.\n",
        "\n",
        "Step-by-Step Working of Bagging Regressor\n",
        "1. Bootstrap Sampling\n",
        "Create multiple random subsets of the training data using sampling with replacement.\n",
        "\n",
        "Each subset is the same size as the original dataset (typically) but includes duplicates and omits some data points.\n",
        "\n",
        "2. Train Base Regressors\n",
        "Train a separate base regressor (commonly a DecisionTreeRegressor) on each bootstrapped sample.\n",
        "\n",
        "You end up with multiple models, each slightly different due to the different data they were trained on.\n",
        "\n",
        "3. Aggregate Predictions\n",
        "For a new input, each regressor makes a prediction.\n",
        "\n",
        "The final prediction is the average of all the individual predictions.\n",
        "\n",
        "*9.  What is the main advantage of ensemble techniques ?*\n",
        "\n",
        "Main Advantage of Ensemble Techniques\n",
        "The main advantage of ensemble techniques is:\n",
        "\n",
        "Improved predictive performance ‚Äî by combining multiple models, ensembles reduce errors like bias, variance, or both, resulting in more accurate and robust predictions than any individual model alone.\n",
        "\n",
        "*10.  What is the main challenge of ensemble methods?*\n",
        "\n",
        "The main challenge of ensemble methods is:\n",
        "\n",
        " Increased complexity and reduced interpretability ‚Äî combining multiple models makes ensembles harder to understand, debug, and maintain compared to single, simple models.\n",
        "\n",
        " Key Challenges of Ensemble Methods\n",
        "\n",
        " | Challenge                           | Explanation                                                             |\n",
        "| ----------------------------------- | ----------------------------------------------------------------------- |\n",
        "|  **Lack of Interpretability**      | Hard to explain why the ensemble made a certain prediction              |\n",
        "|  **Computational Cost**           | Training and predicting with multiple models takes more time and memory |\n",
        "|  **Overfitting Risk in Boosting** | Boosting can sometimes overfit noisy data if not regularized            |\n",
        "|  **Tuning Complexity**            | More hyperparameters (e.g., number of estimators, learning rate)        |\n",
        "|  **Model Management**             | More difficult to deploy and maintain in production                     |\n",
        "|  **Data Requirements**            | Some ensemble techniques may need more data to perform well             |\n",
        "\n",
        "\n",
        "*11.  Explain the key idea behind ensemble techniques?*\n",
        "\n",
        "Key Idea Behind Ensemble Techniques\n",
        "The fundamental idea of ensemble techniques is:\n",
        "\n",
        "Combine multiple models to create a stronger, more accurate, and more robust model than any single one.\n",
        "\n",
        "Different models make different errors. Some might get certain examples wrong, while others get them right.\n",
        "\n",
        "By aggregating their predictions (e.g., voting or averaging), the ensemble reduces the overall error.\n",
        "\n",
        "This diversity among models helps in cancelling out individual mistakes.\n",
        "\n",
        "*12.  What is a Random Forest Classifier*?\n",
        "\n",
        "A Random Forest Classifier is an ensemble learning method used for classification tasks. It builds a forest of decision trees and combines their predictions to improve accuracy and control overfitting.\n",
        "\n",
        "How It Works:\n",
        "\n",
        "Bootstrap Sampling:\n",
        "Multiple decision trees are trained on different random subsets of the training data (with replacement).\n",
        "\n",
        "Feature Randomness:\n",
        "At each split in a tree, only a random subset of features is considered to decide the best split. This introduces diversity among trees.\n",
        "\n",
        "Aggregation (Voting):\n",
        "For classification, each tree makes a prediction, and the class with the majority vote across all trees is chosen as the final output.\n",
        "\n",
        "*13.  What are the main types of ensemble techniques?*\n",
        "\n",
        "Ensemble techniques combine multiple models to improve performance. The three main types are:\n",
        "\n",
        " Bagging (Bootstrap Aggregating)\n",
        "How it works:\n",
        "Train multiple base models independently on different random subsets of the training data (created via bootstrap sampling).\n",
        "\n",
        "Goal:\n",
        "Reduce variance and avoid overfitting.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Random Forest\n",
        "\n",
        "Bagging Classifier/Regressor\n",
        "\n",
        "2. Boosting\n",
        "How it works:\n",
        "Train models sequentially, each new model focusing on correcting errors made by previous models.\n",
        "\n",
        "Goal:\n",
        "Reduce bias and improve overall accuracy by focusing on difficult cases.\n",
        "\n",
        "Examples:\n",
        "\n",
        "AdaBoost\n",
        "\n",
        "Gradient Boosting Machines (GBM)\n",
        "\n",
        "XGBoost\n",
        "\n",
        "LightGBM\n",
        "\n",
        "CatBoost\n",
        "\n",
        "3. Stacking (Stacked Generalization)\n",
        "How it works:\n",
        "Train multiple different base models (can be heterogeneous), then train a meta-model to combine their predictions.\n",
        "\n",
        "Goal:\n",
        "Leverage strengths of diverse models to improve predictions.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Use logistic regression or another model as a meta-learner on top of base learners.\n",
        "\n",
        "*14.  What is ensemble learning in machine learning?*\n",
        "\n",
        "Ensemble Learning is a technique where multiple models (learners) are combined to solve a problem and improve overall performance.\n",
        "\n",
        "Instead of relying on a single model, ensemble learning aggregates predictions from several models to produce a better, more accurate, and robust result.\n",
        "\n",
        "Working :\n",
        "\n",
        "Train multiple models (can be the same type or different).\n",
        "\n",
        "Combine their predictions via methods like:\n",
        "\n",
        "Voting (for classification)\n",
        "\n",
        "Averaging (for regression)\n",
        "\n",
        "More complex strategies like stacking.\n",
        "\n",
        "*15.  When should we avoid using ensemble methods?*\n",
        "\n",
        "While ensemble methods are powerful, there are cases when using them might not be the best choice:\n",
        "\n",
        "Ensembles (especially boosting or stacking) produce complex models.\n",
        "\n",
        "If you need clear, explainable decisions (e.g., medical diagnosis, finance), simple models like logistic regression or single decision trees may be better.\n",
        "\n",
        "1. When Interpretability is Crucial\n",
        "Ensembles (especially boosting or stacking) produce complex models.\n",
        "\n",
        "If you need clear, explainable decisions (e.g., medical diagnosis, finance), simple models like logistic regression or single decision trees may be better.\n",
        "\n",
        "2. Limited Computational Resources\n",
        "Ensembles require more time and memory for training and prediction.\n",
        "\n",
        "On low-resource devices or with strict latency requirements, simpler models might be preferable.\n",
        "\n",
        "3. Small Datasets\n",
        "If your dataset is very small, ensembles might overfit or not provide significant benefit.\n",
        "\n",
        "Sometimes a simple model trained carefully can perform just as well.\n",
        "\n",
        "4. When You Need Quick Prototyping\n",
        "Ensembles add complexity.\n",
        "\n",
        "For fast iteration or proof-of-concept, start with simple models.\n",
        "\n",
        "5. When Base Models Already Perform Well\n",
        "If a single model achieves high accuracy and robustness, ensembles might not add much value but will increase complexity.\n",
        "\n",
        "6. High Maintenance Costs\n",
        "Ensembles can be hard to deploy, debug, and maintain in production.\n",
        "\n",
        "If ease of maintenance is a priority, avoid complex ensembles.\n",
        "\n",
        "*16.  How does Bagging help in reducing overfitting?*\n",
        "\n",
        "Bagging (Bootstrap Aggregating) reduces overfitting mainly by reducing the variance of high-variance models like decision trees.\n",
        "\n",
        "Imagine several weather forecasts from different meteorologists (models). Each may overreact to local noise (like a sudden gust of wind), but averaging their predictions gives a more reliable forecast.\n",
        "\n",
        "Explanation:\n",
        "High Variance Models Overfit Easily:\n",
        "Models like decision trees tend to fit noise in training data, which causes them to perform poorly on unseen data.\n",
        "\n",
        "Bootstrap Sampling Creates Diverse Training Sets:\n",
        "Bagging trains each model on a different random subset (with replacement) of the original data. This means each base model learns a slightly different pattern.\n",
        "\n",
        "Multiple Models Average Out Noise:\n",
        "Since each model makes different errors, averaging their predictions smooths out the noise and reduces fluctuations.\n",
        "\n",
        "Result: Lower Variance Without Increasing Bias Much\n",
        "By averaging many ‚Äúoverfit‚Äù models, bagging stabilizes the overall prediction and prevents any single model‚Äôs noise from dominating.\n",
        "\n",
        "| Effect                 | How Bagging Helps                                                                  |\n",
        "| ---------------------- | ---------------------------------------------------------------------------------- |\n",
        "| Overfitting (variance) | Reduces by averaging multiple overfitting models trained on different data subsets |\n",
        "| Bias                   | Remains roughly the same                                                           |\n",
        "| Stability              | Increases, leading to more robust predictions                                      |\n",
        "\n",
        "\n",
        "*17.  Why is Random Forest better than a single Decision Tree?*\n",
        "\n",
        "Here‚Äôs why a Random Forest is generally better than a single Decision Tree:\n",
        "\n",
        "| Aspect                | Single Decision Tree                    | Random Forest                                                        |\n",
        "| --------------------- | --------------------------------------- | -------------------------------------------------------------------- |\n",
        "| **Overfitting**       | Prone to overfitting the training data  | Reduces overfitting by averaging many trees                          |\n",
        "| **Variance**          | High variance (sensitive to data noise) | Lower variance through ensemble averaging                            |\n",
        "| **Bias**              | Can have low bias but high variance     | Slightly higher bias but much lower variance                         |\n",
        "| **Robustness**        | Sensitive to small changes in data      | More stable and robust to noise and outliers                         |\n",
        "| **Accuracy**          | Usually less accurate on unseen data    | Generally more accurate and generalizes better                       |\n",
        "| **Feature Selection** | Uses all features at splits             | Considers random subsets of features at splits, increasing diversity |\n",
        "| **Interpretability**  | Easier to interpret                     | Harder to interpret due to many trees                                |\n",
        "\n",
        "*18.  What is the role of bootstrap sampling in Bagging?*\n",
        "\n",
        "Role of Bootstrap Sampling in Bagging\n",
        "Bootstrap sampling is the core technique that makes Bagging (Bootstrap Aggregating) work effectively. Here‚Äôs its role:\n",
        "\n",
        "\n",
        "It‚Äôs a method of random sampling with replacement.\n",
        "\n",
        "From the original dataset of size N, you create a new dataset by randomly picking N samples, allowing duplicates.\n",
        "\n",
        "Each bootstrap sample is slightly different from the original data.\n",
        "\n",
        "Why Bootstrap Sampling is Important in Bagging:\n",
        "Creates Diverse Training Sets:\n",
        "Each base model in Bagging is trained on a different bootstrap sample, so models see slightly different data.\n",
        "\n",
        "Introduces Model Diversity:\n",
        "This variation in training data causes each model to learn different patterns and make different errors.\n",
        "\n",
        "Reduces Correlation Between Models:\n",
        "Lower correlation among models is key to effective averaging‚Äîif models make uncorrelated errors, averaging reduces overall error.\n",
        "\n",
        "Enables Out-of-Bag (OOB) Estimation:\n",
        "Since about 1/3 of samples are left out of each bootstrap sample, these ‚Äúleft out‚Äù samples can be used as validation data to estimate performance without a separate test set.\n",
        "\n",
        "*19.  What are some real-world applications of ensemble techniques ?*\n",
        "\n",
        "Ensemble techniques are widely used across industries because of their strong performance and robustness. Here are some real-world applications where ensemble methods shine:\n",
        "\n",
        "Real-World Applications of Ensemble Techniques\n",
        "\n",
        "| Application Area                      | Example Use Cases                                       | Ensemble Technique Often Used         |\n",
        "| ------------------------------------- | ------------------------------------------------------- | ------------------------------------- |\n",
        "| **Finance**                           | Fraud detection, credit scoring, stock price prediction | Random Forest, Gradient Boosting      |\n",
        "| **Healthcare**                        | Disease diagnosis, medical image analysis               | Bagging, Boosting, Stacking           |\n",
        "| **E-commerce**                        | Product recommendation, customer churn prediction       | Gradient Boosting (XGBoost, LightGBM) |\n",
        "| **Marketing**                         | Customer segmentation, campaign response prediction     | Random Forest, AdaBoost               |\n",
        "| **Natural Language Processing (NLP)** | Sentiment analysis, spam detection                      | Voting classifiers, Stacking          |\n",
        "| **Computer Vision**                   | Object detection, facial recognition                    | Ensemble of CNNs, Boosting techniques |\n",
        "| **Weather Forecasting**               | Predicting temperature, rainfall                        | Bagging, Random Forest                |\n",
        "| **Cybersecurity**                     | Intrusion detection, malware classification             | Random Forest, Boosting               |\n",
        "| **Sports Analytics**                  | Player performance prediction, game outcome forecasting | Stacking, Boosting                    |\n",
        "| **Manufacturing**                     | Fault detection, predictive maintenance                 | Random Forest, Gradient Boosting      |\n",
        "\n",
        "*20.  What is the difference between Bagging and Boosting?*\n",
        "\n",
        "Here‚Äôs a clear comparison between Bagging and Boosting:\n",
        "\n",
        "| Aspect                      | Bagging (Bootstrap Aggregating)                                                 | Boosting                                                                      |\n",
        "| --------------------------- | ------------------------------------------------------------------------------- | ----------------------------------------------------------------------------- |\n",
        "| **Goal**                    | Reduce variance by averaging many independent models                            | Reduce bias by sequentially improving weak learners                           |\n",
        "| **Model Training**          | Models trained **independently and in parallel** on different bootstrap samples | Models trained **sequentially**, each focusing on errors of previous models   |\n",
        "| **Data Sampling**           | Random sampling **with replacement** to create diverse datasets                 | Uses the entire dataset but **adjusts weights** to focus on hard examples     |\n",
        "| **Error Correction**        | No focus on correcting previous errors; each model votes equally                | Later models focus on mistakes made by earlier ones, boosting their influence |\n",
        "| **Aggregation Method**      | Simple majority voting (classification) or averaging (regression)               | Weighted sum of model predictions based on their accuracy                     |\n",
        "| **Model Complexity**        | Each model can be complex (e.g., deep trees) but combined to reduce variance    | Usually uses weak learners (e.g., shallow trees) to reduce bias gradually     |\n",
        "| **Common Algorithms**       | Random Forest, Bagging Classifier/Regressor                                     | AdaBoost, Gradient Boosting, XGBoost, LightGBM                                |\n",
        "| **Susceptibility to Noise** | Less sensitive due to averaging over many models                                | More sensitive since boosting focuses on hard (possibly noisy) samples        |\n",
        "| **Parallelization**         | Easy to parallelize since models are independent                                | Harder to parallelize due to sequential training                              |\n"
      ],
      "metadata": {
        "id": "H-PW6TIn2lfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 21.  Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy?\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create base estimator - Decision Tree\n",
        "base_estimator = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Create Bagging Classifier using the Decision Tree as base estimator\n",
        "bagging_clf = BaggingClassifier(base_estimator=base_estimator, n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "V9lCPF2P9VoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 22.  Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE).\n",
        "\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "boston = load_boston()\n",
        "X, y = boston.data, boston.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create base estimator - Decision Tree Regressor\n",
        "base_estimator = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Create Bagging Regressor using the Decision Tree as base estimator\n",
        "bagging_reg = BaggingRegressor(base_estimator=base_estimator, n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = bagging_reg.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Bagging Regressor Mean Squared Error: {mse:.4f}\")\n"
      ],
      "metadata": {
        "id": "VVs4u2xb9h1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 23.  Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(feature_importance_df)\n"
      ],
      "metadata": {
        "id": "PUshE6dv9prI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 24.  Train a Random Forest Regressor and compare its performance with a single Decision Tree\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize single Decision Tree Regressor\n",
        "dt_reg = DecisionTreeRegressor(random_state=42)\n",
        "dt_reg.fit(X_train, y_train)\n",
        "y_pred_dt = dt_reg.predict(X_test)\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "\n",
        "# Initialize Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"Decision Tree Regressor MSE: {mse_dt:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {mse_rf:.4f}\")\n"
      ],
      "metadata": {
        "id": "5iOY9TU492HB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 25.  Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets (optional, OOB uses training data only)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Random Forest Classifier with OOB enabled\n",
        "rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
        "\n",
        "# Train on training data\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Access OOB score\n",
        "print(f\"OOB Score: {rf.oob_score_:.4f}\")\n"
      ],
      "metadata": {
        "id": "F_-llBjk99BL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 26.  Train a Bagging Classifier using SVM as a base estimator and print accuracy\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create base estimator - Support Vector Classifier\n",
        "base_svc = SVC(probability=True, random_state=42)\n",
        "\n",
        "# Create Bagging Classifier with SVM as base estimator\n",
        "bagging_svm = BaggingClassifier(base_estimator=base_svc, n_estimators=20, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = bagging_svm.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier with SVM Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "Px7NSRHB-FzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 27.  Train a Random Forest Classifier with different numbers of trees and compare accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Different number of trees to try\n",
        "n_trees_list = [1, 5, 10, 50, 100, 200]\n",
        "\n",
        "print(\"Number of Trees | Accuracy\")\n",
        "print(\"----------------|---------\")\n",
        "\n",
        "for n_trees in n_trees_list:\n",
        "    rf = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{n_trees:15} | {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "7PLPeaoJ-NBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 28.  Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Base estimator: Logistic Regression (use solver that supports probability)\n",
        "base_lr = LogisticRegression(solver='liblinear', random_state=42)\n",
        "\n",
        "# Bagging Classifier with Logistic Regression\n",
        "bagging_clf = BaggingClassifier(base_estimator=base_lr, n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_probs = bagging_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate AUC score\n",
        "auc_score = roc_auc_score(y_test, y_probs)\n",
        "print(f\"Bagging Classifier with Logistic Regression AUC Score: {auc_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "ETeLNRsQ-Ttj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 29. Train a Random Forest Regressor and analyze feature importance scores.\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "importances = rf_reg.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(feature_importance_df)\n",
        "\n",
        "# Optional: Plot feature importances\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
        "plt.gca().invert_yaxis()\n",
        "plt.title('Feature Importance in Random Forest Regressor')\n",
        "plt.xlabel('Importance Score')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6o3QGDFd-cTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 30.  Train an ensemble model using both Bagging and Random Forest and compare accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Bagging Classifier with Decision Tree base estimator\n",
        "bagging_clf = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_clf.predict(X_test)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "# Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "y_pred_rf = rf_clf.predict(X_test)\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy_bagging:.4f}\")\n",
        "print(f\"Random Forest Classifier Accuracy: {accuracy_rf:.4f}\")\n"
      ],
      "metadata": {
        "id": "NMFUo-L9-mwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid to search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2']\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,\n",
        "                           cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit GridSearch to training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best hyperparameters\n",
        "print(\"Best hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Evaluate the best estimator on test set\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test set accuracy with best RF: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "vt_lmssU-uco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 32.  Train a Bagging Regressor with different numbers of base estimators and compare performance\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Different numbers of base estimators to try\n",
        "n_estimators_list = [1, 5, 10, 50, 100]\n",
        "\n",
        "print(\"Number of Estimators | Mean Squared Error\")\n",
        "print(\"---------------------|--------------------\")\n",
        "\n",
        "for n_estimators in n_estimators_list:\n",
        "    bagging_reg = BaggingRegressor(\n",
        "        base_estimator=DecisionTreeRegressor(random_state=42),\n",
        "        n_estimators=n_estimators,\n",
        "        random_state=42\n",
        "    )\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"{n_estimators:21} | {mse:.4f}\")\n"
      ],
      "metadata": {
        "id": "ZDhe8YY1-4St"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 33.  Train a Random Forest Classifier and analyze misclassified samples\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict test data\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Identify misclassified samples\n",
        "misclassified_indices = np.where(y_pred != y_test)[0]\n",
        "\n",
        "print(f\"Number of misclassified samples: {len(misclassified_indices)}\")\n",
        "\n",
        "# Create a DataFrame for misclassified samples for inspection\n",
        "misclassified_data = pd.DataFrame(X_test[misclassified_indices], columns=feature_names)\n",
        "misclassified_data['True Label'] = y_test[misclassified_indices]\n",
        "misclassified_data['Predicted Label'] = y_pred[misclassified_indices]\n",
        "\n",
        "print(\"\\nMisclassified samples:\")\n",
        "print(misclassified_data)\n"
      ],
      "metadata": {
        "id": "9DmJT59y-_E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 34.  Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train single Decision Tree Classifier\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "dt_clf.fit(X_train, y_train)\n",
        "y_pred_dt = dt_clf.predict(X_test)\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# Train Bagging Classifier with Decision Trees\n",
        "bagging_clf = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging_clf.predict(X_test)\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "print(f\"Single Decision Tree Accuracy: {accuracy_dt:.4f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy_bagging:.4f}\")\n"
      ],
      "metadata": {
        "id": "A3Q0gVeA_HVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 35.  Train a Random Forest Classifier and visualize the confusion matrix\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = rf.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize confusion matrix with seaborn heatmap\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=data.target_names, yticklabels=data.target_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix - Random Forest Classifier')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Q7iuuWzD_VXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 36.  Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Base estimators\n",
        "estimators = [\n",
        "    ('dt', DecisionTreeClassifier(random_state=42)),\n",
        "    ('svm', SVC(probability=True, random_state=42)),\n",
        "    ('lr', LogisticRegression(solver='liblinear', random_state=42))\n",
        "]\n",
        "\n",
        "# Stacking Classifier with Logistic Regression as final estimator\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression(random_state=42)\n",
        ")\n",
        "\n",
        "# Train stacking classifier\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test)\n",
        "accuracy_stack = accuracy_score(y_test, y_pred_stack)\n",
        "\n",
        "# Train individual models for comparison\n",
        "accuracies = {}\n",
        "for name, model in estimators:\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracies[name] = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Individual Model Accuracies:\")\n",
        "for name, acc in accuracies.items():\n",
        "    print(f\"{name}: {acc:.4f}\")\n",
        "\n",
        "print(f\"\\nStacking Classifier Accuracy: {accuracy_stack:.4f}\")\n"
      ],
      "metadata": {
        "id": "p4aBVcA7_cpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 37.  Train a Random Forest Classifier and print the top 5 most important features\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame with feature names and their importance scores\n",
        "feat_imp_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort by importance descending and get top 5\n",
        "top5_features = feat_imp_df.sort_values(by='Importance', ascending=False).head(5)\n",
        "\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(top5_features)\n"
      ],
      "metadata": {
        "id": "eBZWQHyX_kSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 38.  Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Bagging Classifier with Decision Trees\n",
        "bagging_clf = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = bagging_clf.predict(X_test)\n",
        "\n",
        "# Evaluate Precision, Recall, and F1-score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-score:  {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "3Zu_ofCs_rvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 39.  Train a Random Forest Classifier and analyze the effect of max_depth on accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "max_depth_values = [1, 2, 4, 6, 8, 10, 15, 20, None]\n",
        "accuracies = []\n",
        "\n",
        "for depth in max_depth_values:\n",
        "    rf = RandomForestClassifier(max_depth=depth, n_estimators=100, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(acc)\n",
        "    print(f\"max_depth={str(depth):>4} --> Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Plotting the effect of max_depth on accuracy\n",
        "plt.figure(figsize=(8,5))\n",
        "depth_labels = [str(d) for d in max_depth_values]\n",
        "plt.plot(depth_labels, accuracies, marker='o')\n",
        "plt.title('Effect of max_depth on Random Forest Accuracy')\n",
        "plt.xlabel('max_depth')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jAO3bE39_y97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance.\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Bagging Regressor with Decision Tree base estimator\n",
        "bagging_tree = BaggingRegressor(\n",
        "    base_estimator=DecisionTreeRegressor(random_state=42),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_tree.fit(X_train, y_train)\n",
        "y_pred_tree = bagging_tree.predict(X_test)\n",
        "mse_tree = mean_squared_error(y_test, y_pred_tree)\n",
        "\n",
        "# Initialize Bagging Regressor with K-Neighbors base estimator\n",
        "bagging_knn = BaggingRegressor(\n",
        "    base_estimator=KNeighborsRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_knn.fit(X_train, y_train)\n",
        "y_pred_knn = bagging_knn.predict(X_test)\n",
        "mse_knn = mean_squared_error(y_test, y_pred_knn)\n",
        "\n",
        "print(f\"Bagging with Decision Tree Regressor MSE: {mse_tree:.4f}\")\n",
        "print(f\"Bagging with K-Neighbors Regressor MSE: {mse_knn:.4f}\")\n"
      ],
      "metadata": {
        "id": "AQ03WWO__7m9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 41.  Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for positive class\n",
        "y_proba = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "ta-ZB-LAAEPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 42.  Train a Bagging Classifier and evaluate its performance using cross-validatio\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Initialize Bagging Classifier with Decision Trees\n",
        "bagging_clf = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "cv_scores = cross_val_score(bagging_clf, X, y, cv=5, scoring='accuracy')\n",
        "\n",
        "print(f\"Cross-validation accuracy scores: {cv_scores}\")\n",
        "print(f\"Mean accuracy: {cv_scores.mean():.4f}\")\n",
        "print(f\"Standard deviation: {cv_scores.std():.4f}\")\n"
      ],
      "metadata": {
        "id": "0iCEHcv_APPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 43.  Train a Random Forest Classifier and plot the Precision-Recall curv\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for positive class\n",
        "y_scores = rf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute precision-recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "# Compute average precision score\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(recall, precision, label=f'Average Precision = {avg_precision:.4f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve - Random Forest Classifier')\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bbBJGlToAVmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # 44.  Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Base estimators\n",
        "estimators = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('lr', LogisticRegression(max_iter=1000, random_state=42))\n",
        "]\n",
        "\n",
        "# Stacking Classifier with Logistic Regression as final estimator\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=estimators,\n",
        "    final_estimator=LogisticRegression(random_state=42)\n",
        ")\n",
        "\n",
        "# Train stacking classifier\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "y_pred_stack = stacking_clf.predict(X_test)\n",
        "accuracy_stack = accuracy_score(y_test, y_pred_stack)\n",
        "\n",
        "# Train individual models for comparison\n",
        "accuracies = {}\n",
        "for name, model in estimators:\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracies[name] = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Individual Model Accuracies:\")\n",
        "for name, acc in accuracies.items():\n",
        "    print(f\"{name}: {acc:.4f}\")\n",
        "\n",
        "print(f\"\\nStacking Classifier Accuracy: {accuracy_stack:.4f}\")\n"
      ],
      "metadata": {
        "id": "Jf2YVcaIAdWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # 45.  Train a Bagging Regressor with different levels of bootstrap samples and compare performance.\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Different levels of bootstrap samples to try (as fractions of the training set)\n",
        "max_samples_values = [0.3, 0.5, 0.7, 1.0]\n",
        "\n",
        "mse_scores = []\n",
        "\n",
        "for max_samples in max_samples_values:\n",
        "    bagging_reg = BaggingRegressor(\n",
        "        base_estimator=DecisionTreeRegressor(random_state=42),\n",
        "        n_estimators=50,\n",
        "        max_samples=max_samples,\n",
        "        bootstrap=True,\n",
        "        random_state=42\n",
        "    )\n",
        "    bagging_reg.fit(X_train, y_train)\n",
        "    y_pred = bagging_reg.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores.append(mse)\n",
        "    print(f\"max_samples={max_samples:.1f} --> MSE: {mse:.4f}\")\n",
        "\n",
        "# Plot MSE vs max_samples\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot([str(ms) for ms in max_samples_values], mse_scores, marker='o')\n",
        "plt.title(\"Effect of max_samples on Bagging Regressor Performance\")\n",
        "plt.xlabel(\"max_samples (fraction of training data)\")\n",
        "plt.ylabel(\"Mean Squared Error\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HNVZKP17Akyx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}